/**************************************************************************//**
 *
 * \file   fulllayernet.cc
 * \author Daniel Strigl, Klaus Kofler
 * \date   Jun 04 2009
 *
 * $Id$
 *
 * \brief  Implementation of cnnplus::FullLayerNet.
 *
 *****************************************************************************/

#include "fulllayernet.hh"
#include "error.hh"
#include "matvecli.hh"
#include <sstream>
#include <cstdarg>
#include <typeinfo>
#ifdef CNNPLUS_MATLAB_FOUND
#include <mat.h>
#endif // CNNPLUS_MATLAB_FOUND

CNNPLUS_NS_BEGIN

template<typename T, class SquFnc>
FullLayerNet<T, SquFnc>::FullLayerNet(size_t const sizeIn, size_t const numLayers, ...)
{
    if (sizeIn < 1)
        throw ParameterError("sizeIn", "must be greater or equal to one.");
    else if (numLayers < 1)
        throw ParameterError("numLayers", "must be greater or equal to one.");

    // TODO more checks for validity, throw exceptions!
    va_list ap; va_start(ap, numLayers);
    for (size_t i = 0, size = sizeIn; i < numLayers; ++i)
    {
        // Create layer
        layers_.push_back(new FullLayer<T, SquFnc>(
            Size(1, size), Size(1, va_arg(ap, size_t))));
        size = layers_.back()->sizeOut().area();
        CNNPLUS_ASSERT(size > 0);
        // Allocate memory
        if (i < numLayers - 1)
            states_.push_back(matvecli::allocv<T>(size));
    }
    va_end(ap);
}

template<typename T, class SquFnc>
FullLayerNet<T, SquFnc>::~FullLayerNet()
{
    // Delete layers
    for (size_t i = 0; i < layers_.size(); ++i)
        delete layers_[i];

    // Deallocate memory
    for (size_t i = 0; i < states_.size(); ++i)
        matvecli::free<T>(states_[i]);
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::forget(T const sigma, bool scale)
{
    matvecli::randreseed();

    for (size_t i = 0; i < layers_.size(); ++i)
        layers_[i]->forget(sigma, scale);
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::forget()
{
    matvecli::randreseed();

    for (size_t i = 0; i < layers_.size(); ++i)
        layers_[i]->forget();
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::reset()
{
    for (size_t i = 0; i < layers_.size(); ++i)
        layers_[i]->reset();
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::update(T const eta)
{
    for (size_t i = 0; i < layers_.size(); ++i)
        layers_[i]->update(eta);
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::fprop(T const * in, T * out)
{
    CNNPLUS_ASSERT(in); // 'out' can be NULL

    // For networks with only one layer
    if (layers_.size() == 1) {
        layers_[0]->fprop(in, layers_[0]->sizeIn().area(), out, layers_[0]->sizeOut().area());
    }
    else {
        // ... and for networks with more than one layer
        for (int i = 0; i < static_cast<int>(layers_.size()); ++i)
        {
            size_t const strideIn = layers_[i]->sizeIn().area();
            size_t const strideOut = layers_[i]->sizeOut().area();

            if (i == 0) {
                // First layer
                layers_[i]->fprop(in, strideIn, states_[i], strideOut);
            }
            else if (i < static_cast<int>(layers_.size() - 1)) {
                // Layers between
                layers_[i]->fprop(states_[i-1], strideIn, states_[i], strideOut);
            }
            else {
                // Last layer
                layers_[i]->fprop(states_[i-1], strideIn, out, strideOut);
            }
        }
    }
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::bprop(T * in, T const * out, bool accumGradients)
{
    CNNPLUS_ASSERT(out); // 'in' can be NULL

    // For networks with only one layer
    if (layers_.size() == 1) {
        layers_[0]->bprop(in, layers_[0]->sizeIn().area(), out, layers_[0]->sizeOut().area());
    }
    else {
        // ... and for networks with more than one layer
        for (int i = static_cast<int>(layers_.size() - 1); i >= 0; --i)
        {
            size_t const strideIn = layers_[i]->sizeIn().area();
            size_t const strideOut = layers_[i]->sizeOut().area();

            if (i == static_cast<int>(layers_.size() - 1)) {
                // Last layer
                layers_[i]->bprop(states_[i-1], strideIn, out, strideOut);
            }
            else if (i > 0) {
                // Layers between
                layers_[i]->bprop(states_[i-1], strideIn, states_[i], strideOut);
            }
            else {
                // First layer
                layers_[i]->bprop(in, strideIn, states_[i], strideOut);
            }
        }
    }
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::load(std::string const & filename)
{
#ifdef CNNPLUS_MATLAB_FOUND
    MATFile * file = matOpen(filename.c_str(), "r");
    if (!file)
        throw MatlabError("Failed to open '" + filename + "'.");

    mxArray * arrLayer = NULL;

    try {
        if (!(arrLayer = matGetVariable(file, "layer")) || !mxIsCell(arrLayer))
            throw MatlabError("Failed to read 'layer'.");

        for (size_t i = 0; i < layers_.size(); ++i) {
            std::stringstream ss;
            ss << "layer" << (i + 1);
            layers_[i]->load(mxGetCell(arrLayer, i));
        }
    }
    catch (...) {
        if (arrLayer) mxDestroyArray(arrLayer);
        matClose(file);
        throw;
    }

    mxDestroyArray(arrLayer);
    matClose(file);
#else
    throw NotImplementedError("MATLAB not found, function 'load' not supported.");
#endif // CNNPLUS_MATLAB_FOUND
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::save(std::string const & filename) const
{
#ifdef CNNPLUS_MATLAB_FOUND
    MATFile * file = matOpen(filename.c_str(), "w");
    if (!file)
        throw MatlabError("Failed to create '" + filename + "'.");

    mxArray * arrLayer = NULL;

    try {
        if (!(arrLayer = mxCreateCellMatrix(layers_.size(), 1)))
            throw MatlabError("Failed to create array.");

        for (size_t i = 0; i < layers_.size(); ++i) {
            std::stringstream ss;
            ss << "layer" << (i + 1);
            mxSetCell(arrLayer, i, layers_[i]->save());
        }

        if (matPutVariable(file, "layer", arrLayer))
            throw MatlabError("Failed to write 'layer'.");
    }
    catch (...) {
        if (arrLayer) mxDestroyArray(arrLayer);
        matClose(file);
        throw;
    }

    mxDestroyArray(arrLayer);
    matClose(file);
#else
    throw NotImplementedError("MATLAB not found, function 'save' not supported.");
#endif // CNNPLUS_MATLAB_FOUND
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::writeOut(std::string const & filename, T * const out) const
{
#ifdef CNNPLUS_MATLAB_FOUND
    MATFile * file = matOpen(filename.c_str(), "w");
    if (!file)
        throw MatlabError("Failed to create '" + filename + "'.");

    mxArray * arrLayer = NULL;

    try {
        if (!(arrLayer = mxCreateCellMatrix(out ? layers_.size() : layers_.size() - 1, 1)))
            throw MatlabError("Failed to create array.");

        for (size_t i = 0; i < layers_.size(); ++i) {
            if (i < layers_.size() - 1)
                mxSetCell(arrLayer, i, layers_[i]->writeOut(states_[i], layers_[i]->sizeOut().area()));
            else if (out)
                mxSetCell(arrLayer, i, layers_[i]->writeOut(out, layers_[i]->sizeOut().area()));
        }

        if (matPutVariable(file, "layer", arrLayer))
            throw MatlabError("Failed to write 'layer'.");
    }
    catch (...) {
        if (arrLayer) mxDestroyArray(arrLayer);
        matClose(file);
        throw;
    }

    mxDestroyArray(arrLayer);
    matClose(file);
#else
    throw NotImplementedError("MATLAB not found, function 'writeOut' not supported.");
#endif // CNNPLUS_MATLAB_FOUND
}

template<typename T, class SquFnc> void
FullLayerNet<T, SquFnc>::trainableParam(typename NeuralNet<T>::TrainableParam & param)
{
    param.resize(layers_.size());
    for (size_t i = 0; i < layers_.size(); ++i)
        layers_[i]->trainableParam(param[i]);
}

template<typename T, class SquFnc> std::string
FullLayerNet<T, SquFnc>::toString() const
{
    std::stringstream ss;
    ss << "FullLayerNet<" << typeid(T).name() << ">[";
    for (size_t i = 0; i < layers_.size(); ++i) {
        ss << "L" << (i + 1) << ":" << layers_[i]->toString();
        ss << (i < (layers_.size() - 1) ? "," : "]");
    }
    return ss.str();
}

template<typename T, class SquFnc> size_t
FullLayerNet<T, SquFnc>::numTrainableParam() const
{
    size_t numTrainableParam = 0;
    for (size_t i = 0; i < layers_.size(); ++i)
        numTrainableParam += layers_[i]->numTrainableParam();
    return numTrainableParam;
}

template<typename T, class SquFnc> size_t
FullLayerNet<T, SquFnc>::numConnections() const
{
    size_t numConnections = 0;
    for (size_t i = 0; i < layers_.size(); ++i)
        numConnections += layers_[i]->numConnections();
    return numConnections;
}

/*! \addtogroup eti_grp Explicit Template Instantiation
 @{
 */
template class FullLayerNet< float,  Tanh<float>        >;
template class FullLayerNet< float,  StdSigmoid<float>  >;
template class FullLayerNet< float,  LogSigmoid<float>  >;
template class FullLayerNet< float,  Identity<float>    >;

template class FullLayerNet< double, Tanh<double>       >;
template class FullLayerNet< double, StdSigmoid<double> >;
template class FullLayerNet< double, LogSigmoid<double> >;
template class FullLayerNet< double, Identity<double>   >;
/*! @} */

CNNPLUS_NS_END
